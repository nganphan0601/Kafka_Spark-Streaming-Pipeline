
# Activate venv 
source venv/bin/activate

# start the spark-project container
docker start -ai spark-project 

# submit spark job to read streaming data from kafka

docker container stop spark-project || true
docker container rm spark-project || true

docker run -it --rm \
--name spark-project \
--network=streaming-network \
-p 4040:4040 \
-v /home/ngan/DE/Kafka_Spark-Streaming-Pipeline:/spark \
-v spark_lib:/opt/bitnami/spark/.ivy2 \
-e PYSPARK_PYTHON=./environment/bin/python \
-e PYSPARK_DRIVER_PYTHON=python \
unigap/spark:3.5 bash -c "
python -m venv environment &&
  source environment/bin/activate &&
  pip install -r /spark/requirements.txt &&
  venv-pack -o environment.tar.gz &&
spark-submit \
    --conf spark.executor.instances=2 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1,org.postgresql:postgresql:42.7.3 \
    --archives environment.tar.gz#environment \
    /spark/main.py
"



kafka-console-consumer \
  --bootstrap-server 46.202.167.130:9094 \
  --topic product_view \
  --from-beginning \
  --consumer-property security.protocol=SASL_PLAINTEXT \
  --consumer-property sasl.mechanism=PLAIN \
  --consumer-property sasl.jaas.config='org.apache.kafka.common.security.plain.PlainLoginModule required username="kafka" password="UnigapKafka@2024";'


